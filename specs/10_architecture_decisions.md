# Architecture Decision Records (ADRs)

## ADR-001: Vector Database Selection - ChromaDB vs Qdrant

**Date**: 2025-10-12

**Status**: Accepted

**Context**: 
The system requires a vector database to store and search document embeddings for the RAG pipeline. Two main options were considered: ChromaDB and Qdrant.

**Decision**: 
Use ChromaDB as the vector database for v1.

**Rationale**:

**ChromaDB Advantages:**
- **Resource Efficiency**: Embedded mode runs as Python library (~100-200MB vs Qdrant's 500MB+ overhead)
- **Simplicity**: Single-process deployment, no separate server to manage
- **Pi5 Optimization**: Better suited for resource-constrained single-board computers
- **Storage**: SQLite backend provides reliable, file-based persistence
- **Proven**: Successfully used in the reference implementation ("A Local RAG on a RasPi")
- **Dependencies**: Fewer external dependencies, simpler Docker setup

**Qdrant Disadvantages for This Use Case:**
- **Resource Overhead**: Requires separate server process, higher memory usage
- **Complexity**: Additional networking, service management, and configuration
- **Over-engineering**: Performance benefits not needed for 100s of documents
- **Pi5 Constraints**: Competes with LLM for limited CPU/RAM resources

**Consequences**:
- Simpler deployment and maintenance
- Lower resource usage on Pi5
- Single point of failure (embedded in application)
- Migration path available if scaling needs change

**Future Considerations**:
- Qdrant remains an option for future scaling requirements
- Easy migration path exists due to similar APIs
- Performance monitoring will inform future decisions

---

## ADR-002: LLM Execution Strategy - CPU vs NPU

**Date**: 2025-10-12

**Status**: Accepted

**Context**: 
The system needs to run LLM inference locally. Options considered include CPU-only execution, NPU acceleration (Hailo-10H), and hybrid approaches.

**Decision**: 
Use CPU-only execution via llama-cpp-python for v1, with NPU support planned for future milestones.

**Rationale**:

**CPU Execution Advantages:**
- **Simplicity**: Well-established toolchain with llama.cpp and Python bindings
- **Compatibility**: Works on both Pi5 and AMD64 without hardware dependencies
- **Ecosystem**: Large community, extensive model support, good documentation
- **Development Speed**: Faster iteration without hardware-specific complications

**NPU Challenges for v1:**
- **Complexity**: Additional hardware dependency and driver requirements
- **Limited Support**: Fewer models and tools support Hailo acceleration
- **Development Time**: Longer integration and testing cycles
- **Portability**: Reduces cross-platform compatibility

**Consequences**:
- Longer inference times on Pi5 (acceptable for v1 use case)
- Higher power consumption during inference
- Simpler development and testing workflow
- Clear upgrade path for NPU integration in future milestones

**Future Considerations**:
- NPU integration planned for milestone 4-5
- Will evaluate performance gains vs complexity trade-offs
- May support both CPU and NPU modes for user choice

---

## ADR-003: Deployment Architecture - Embedded vs Service-Based

**Date**: 2025-10-12

**Status**: Accepted

**Context**: 
The system architecture needs to balance ease of deployment, resource efficiency, and maintainability. Considered approaches include Docker containers, separate services (e.g., external Ollama), and embedded components.

**Decision**: 
Use a single Python process with embedded LLM and vector database, deployed via APT package with systemd service management.

**Rationale**:

**Embedded Architecture Advantages:**
- **Deployment Simplicity**: Single APT package installation with `sudo apt install ./local-rag.deb`
- **Resource Efficiency**: No inter-process communication overhead, shared memory space
- **Reliability**: Fewer points of failure, simpler dependency management
- **User Experience**: One command installation, standard systemd service management
- **Cross-platform**: Same architecture works on Pi5 and AMD64

**Service-Based Disadvantages:**
- **Complexity**: Multiple services to install, configure, and manage
- **Resource Overhead**: Network communication, separate process memory overhead
- **Installation Friction**: Users must manage multiple components and configurations
- **Networking Issues**: Docker-to-host communication complications on Pi5

**Implementation Details:**
- Single Python process containing FastAPI, llama-cpp-python, and ChromaDB
- Systemd service for lifecycle management
- Configuration via `/etc/local-rag/config.yaml`
- Data storage in `/var/lib/local-rag/`
- Logs via systemd journal and optional file output

**Consequences**:
- Easier user installation and maintenance
- Slightly less modular than service-based approach
- All components share same process lifecycle
- Future modularization possible if needed

**Future Considerations**:
- Plugin architecture can provide modularity within single process
- Performance monitoring will inform need for service separation
- Container deployment option can be added later if requested

---

## ADR-004: LLM Integration - Ollama vs llama-cpp-python

**Date**: 2025-10-12

**Status**: Accepted

**Context**: 
The system needs to integrate with LLM inference. Two primary options were considered: using Ollama as an external service or embedding llama-cpp-python directly in the application.

**Decision**: 
Use llama-cpp-python directly embedded in the Python application.

**Rationale**:

**llama-cpp-python Advantages:**
- **Deployment Simplicity**: No external service dependency, single process deployment
- **Resource Efficiency**: No network overhead, direct memory access, shared process space
- **Developer Experience**: Native Python API, familiar patterns, good documentation
- **Cross-platform**: Pre-built wheels for both ARM64 (Pi5) and AMD64
- **Control**: Fine-grained control over model loading, inference parameters, and memory management

**Ollama Disadvantages for This Use Case:**
- **Service Complexity**: Requires separate service installation and management
- **Network Overhead**: HTTP API calls add latency and complexity
- **Resource Usage**: Additional process overhead on resource-constrained Pi5
- **Installation Friction**: Users must install and configure two separate components

**Technical Implementation:**
- Direct integration with llama-cpp-python Python bindings
- Model files stored in `/var/lib/local-rag/models/`
- Configurable inference parameters via config file
- Streaming token generation for responsive UI
- Error handling and retry logic within application

**Consequences**:
- Simpler deployment and user experience
- Better resource utilization on Pi5
- More complex model management (no Ollama's model download features)
- Direct dependency on llama-cpp-python stability

**Future Considerations**:
- Can add Ollama support as alternative backend if requested
- Model management tools may be needed for user convenience
- Performance comparison with Ollama can inform future decisions