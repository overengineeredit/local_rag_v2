server:
  host: "127.0.0.1"
  port: 8080
  workers: 1
  reload: false

storage:
  data_dir: "./data"
  models_dir: "./models"
  vector_db_dir: "./data/chromadb"

llm:
  # Current active model (uncomment one of the options below to switch)
  model_path: "./models/deepseek-r1-distill-qwen-1.5b.Q4_K_M.gguf"
  
  # Available model options (download instructions in README.md):
  # 
  # DeepSeek-R1-Distill-Qwen-1.5B (1.1GB) - Fast, efficient reasoning, good for RAG
  # model_path: "./models/deepseek-r1-distill-qwen-1.5b.Q4_K_M.gguf"
  #
  # Qwen2.5 3B Instruct (1.8GB) - Excellent for technical content, coding, RAG
  # model_path: "./models/qwen2.5-3b-instruct.Q4_K_M.gguf"
  #
  # Llama 3.2 3B Instruct (1.9GB) - Meta's latest, balanced performance, reliable
  # model_path: "./models/llama-3.2-3b-instruct.Q4_K_M.gguf"
  #
  # Phi-3.5 Mini (2.3GB) - Microsoft's efficient architecture, good reasoning
  # model_path: "./models/phi-3.5-mini-instruct.Q4_K_M.gguf"
  #
  # Gemma 2 2B (1.6GB) - Google's optimized model, creative responses
  # model_path: "./models/gemma-2-2b-it.Q4_K_M.gguf"
  #
  # TinyLlama 1.1B (638MB) - Ultra fast, minimal memory, good for testing
  # model_path: "./models/tinyllama-1.1b-chat.Q4_K_M.gguf"
  
  context_length: 2048
  temperature: 0.7
  max_tokens: 512
  n_threads: 4

embedding:
  model: "all-MiniLM-L6-v2"
  batch_size: 32

content:
  chunk_size: 512
  chunk_overlap: 50
  max_file_size_mb: 100

logging:
  level: "INFO"
  format: "json"
  file: "./data/logs/app.log"
  max_size: "10MB"
  backup_count: 5